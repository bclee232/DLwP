{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_embed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZcMICOmO3TtJKnScrbOwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bclee232/DLwP/blob/master/8_embed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E44vkWRs6C56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "35e4bb87-a10e-4cc5-9af8-139618f635ba"
      },
      "source": [
        "# word level one-hot encoding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The quick brown fox.', 'The cat ate the homework.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000) # creates tokenizer for 1000 most common words\n",
        "tokenizer.fit_on_texts(samples) # builds the word index\n",
        "seq = tokenizer.texts_to_sequences(samples) # turns str in samples into list of int indices\n",
        "one_hot_res = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "word_ind = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_ind))\n",
        "print('word_ind:', word_ind, 'one_hot_res', one_hot_res, 'seq:', seq, 'tokenizer:', tokenizer)\n",
        "# \"the\" and \"The\" are the same words?"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7 unique tokens.\n",
            "word_ind: {'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'cat': 5, 'ate': 6, 'homework': 7} one_hot_res [[0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]] seq: [[1, 2, 3, 4], [1, 5, 6, 1, 7]] tokenizer: <keras_preprocessing.text.Tokenizer object at 0x7fa486fc1cc0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNg3QYUk8G79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "39f22b1c-60b5-4b82-ee28-c7140272eb32"
      },
      "source": [
        "# one-hot hashing trick\n",
        "import numpy as np\n",
        "\n",
        "dimensionality = 100 # dimensionality >>> max_len to min. hash collisions\n",
        "max_len = 10\n",
        "\n",
        "res = np.zeros((len(samples), max_len, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_len]:\n",
        "    ind = abs(hash(word)) % dimensionality\n",
        "    res[i, j, ind] = 1\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBjsWWWD_wro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5c392edd-02dc-4345-c6d7-e574a76892ea"
      },
      "source": [
        "# load and preprocess IMDB data for Embedding layer\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "(train_data, train_targets), (test_data, test_targets) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "max_len = 20\n",
        "# convert to 2D int tensor of shape (samples, seq_len)\n",
        "x_train = preprocessing.sequence.pad_sequences(train_data,\n",
        "                                               maxlen=max_len)\n",
        "x_test = preprocessing.sequence.pad_sequences(test_data,\n",
        "                                              maxlen=max_len)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jp8_ONmChtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "092b4eb5-f603-41d5-c6aa-0dfdba6d6667"
      },
      "source": [
        "# Embedding layer with Dense layer on top\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "dim = 8\n",
        "model = Sequential()\n",
        "# make activations to 3D tensor of shape (max_features, max_len, dim)\n",
        "model.add(Embedding(max_features, dim, input_length=max_len))\n",
        "# flatten to 2D tensor of shape (max_featuers, max_len * dim)\n",
        "model.add(Flatten())\n",
        "# add Dense classifier on top\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lFrpAH7Dn6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "08e6af03-8aa0-456e-9db1-0b2f395ecc13"
      },
      "source": [
        "hist = model.fit(x_train, train_targets, batch_size=32, epochs=10, \n",
        "                 validation_split=0.2)\n",
        "# peak 76% val_acc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 1s 74us/step - loss: 0.6684 - acc: 0.6267 - val_loss: 0.6156 - val_acc: 0.7044\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 1s 59us/step - loss: 0.5374 - acc: 0.7551 - val_loss: 0.5224 - val_acc: 0.7340\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 1s 55us/step - loss: 0.4586 - acc: 0.7890 - val_loss: 0.4986 - val_acc: 0.7480\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 1s 56us/step - loss: 0.4198 - acc: 0.8114 - val_loss: 0.4939 - val_acc: 0.7530\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 1s 55us/step - loss: 0.3930 - acc: 0.8247 - val_loss: 0.4944 - val_acc: 0.7564\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 1s 59us/step - loss: 0.3708 - acc: 0.8375 - val_loss: 0.4967 - val_acc: 0.7590\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 1s 58us/step - loss: 0.3511 - acc: 0.8494 - val_loss: 0.5015 - val_acc: 0.7600\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 1s 58us/step - loss: 0.3329 - acc: 0.8582 - val_loss: 0.5084 - val_acc: 0.7574\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 1s 57us/step - loss: 0.3155 - acc: 0.8674 - val_loss: 0.5186 - val_acc: 0.7516\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 1s 55us/step - loss: 0.2994 - acc: 0.8771 - val_loss: 0.5283 - val_acc: 0.7486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnIBqLC0KlT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86e655e7-fc0b-40b0-a305-0adb505d1538"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ47EnD0LTQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82af081f-8bfc-4ec1-c0c9-3f8de62c073b"
      },
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content/drive/My Drive/imdb_small'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "                \n",
        "print(len(labels), len(texts))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "202 202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_qLbo8TMIho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39db2db6-be29-4aba-bb26-738ec0049650"
      },
      "source": [
        "# tokenise the raw texts\n",
        "maxlen = 102 # cuts off reviews after 102 words\n",
        "training_samp = 100\n",
        "valid_samp = 100\n",
        "max_words = 10002 # top 10002 words in dataset\n",
        "\n",
        "t = Tokenizer(max_words)\n",
        "t.fit_on_texts(texts)\n",
        "seq = t.texts_to_sequences(texts)\n",
        "# no word matrix?\n",
        "word_ind = t.word_index\n",
        "print(len(word_ind))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z45xVdHNa7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78559173-7568-43e1-c586-0d753f7944bf"
      },
      "source": [
        "data = preprocessing.sequence.pad_sequences(seq, maxlen=maxlen)\n",
        "lab = np.asarray(labels)\n",
        "print(data.shape, lab.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(202, 102) (202,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTo9nZfcNs5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = np.arange(data.shape[0])\n",
        "# shuffle data\n",
        "np.random.shuffle(i)\n",
        "data = data[i]\n",
        "lab = lab[i]\n",
        "# split data into training and validation\n",
        "x_train = data[:training_samp]\n",
        "y_train = lab[:training_samp]\n",
        "x_val = data[training_samp:training_samp+valid_samp]\n",
        "y_val = lab[training_samp:training_samp+valid_samp]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nKy8FghOV3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using task specific embedding with small input as baseline\n",
        "dim = 8\n",
        "m = Sequential()\n",
        "# make activations to 3D tensor of shape (max_features, max_len, dim)\n",
        "m.add(Embedding(max_words, dim, input_length=maxlen))\n",
        "# flatten to 2D tensor of shape (max_featuers, max_len * dim)\n",
        "m.add(Flatten())\n",
        "# add Dense classifier on top\n",
        "m.add(Dense(1, activation='sigmoid'))\n",
        "m.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1M6h4aGRfMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "52ab6c3b-59fd-4917-e429-4f1726f8eed6"
      },
      "source": [
        "history = m.fit(x_train, y_train, 30, 10, validation_data=(x_val, y_val))\n",
        "# val_acc of 53%"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 100 samples, validate on 100 samples\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.6932 - acc: 0.4600 - val_loss: 0.6936 - val_acc: 0.5100\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 0s 166us/step - loss: 0.6654 - acc: 0.9400 - val_loss: 0.6934 - val_acc: 0.5100\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 0s 180us/step - loss: 0.6459 - acc: 0.9700 - val_loss: 0.6933 - val_acc: 0.5200\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 0s 153us/step - loss: 0.6287 - acc: 0.9700 - val_loss: 0.6931 - val_acc: 0.5300\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 0s 195us/step - loss: 0.6115 - acc: 0.9800 - val_loss: 0.6931 - val_acc: 0.5200\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 0s 186us/step - loss: 0.5943 - acc: 0.9800 - val_loss: 0.6930 - val_acc: 0.5100\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 183us/step - loss: 0.5767 - acc: 0.9800 - val_loss: 0.6930 - val_acc: 0.5100\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 0s 147us/step - loss: 0.5585 - acc: 0.9800 - val_loss: 0.6929 - val_acc: 0.5300\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 0s 144us/step - loss: 0.5395 - acc: 0.9800 - val_loss: 0.6930 - val_acc: 0.5200\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 0s 146us/step - loss: 0.5199 - acc: 0.9800 - val_loss: 0.6930 - val_acc: 0.5100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYrfOMeeR8Y_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d151eef-c726-42da-f288-d97ce8341390"
      },
      "source": [
        "# parse in glove embedding file\n",
        "path = '/content/drive/My Drive/glove/glove.6B.100d.txt'\n",
        "embeddings_index = {}\n",
        "f = open(path)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYHqQyAKSrWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load embedding matrix\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_ind.items():\n",
        "  if i < max_words:\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkB1P-C2TnQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0d582d6b-d10f-4a9e-926e-3956daa01190"
      },
      "source": [
        "# define net architecture\n",
        "net = Sequential()\n",
        "net.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "net.add(Flatten())\n",
        "net.add(Dense(32, activation='relu'))\n",
        "net.add(Dense(1, activation='sigmoid'))\n",
        "net.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 102, 100)          1000200   \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10200)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                326432    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,326,665\n",
            "Trainable params: 1,326,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAdR5-TST5u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Glove into network\n",
        "first = net.layers[0]\n",
        "first.set_weights([embedding_matrix])\n",
        "first.trainable = False"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpD0HSd6Ufoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "232c8a69-08d9-43f0-e1c9-e72853d8955b"
      },
      "source": [
        "# train and eval\n",
        "net.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "net.fit(x_train, y_train, batch_size=32, epochs=10, \n",
        "        validation_data=(x_val, y_val))\n",
        "net.save_weights('pre_trained_glove_model.h5')\n",
        "# peak accuracy of 54% (slightly better than task-specific?)\n",
        "# test on actual test data (but takes time to load)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100 samples, validate on 100 samples\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.5223 - acc: 0.4800 - val_loss: 0.6893 - val_acc: 0.4900\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 0s 277us/step - loss: 0.5966 - acc: 0.7300 - val_loss: 0.6998 - val_acc: 0.5300\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 0s 288us/step - loss: 0.3928 - acc: 0.8600 - val_loss: 1.1423 - val_acc: 0.5100\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 0s 306us/step - loss: 0.3808 - acc: 0.8700 - val_loss: 0.7070 - val_acc: 0.5400\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 0s 298us/step - loss: 0.2763 - acc: 0.9000 - val_loss: 1.0411 - val_acc: 0.5100\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 0s 288us/step - loss: 0.1802 - acc: 0.9400 - val_loss: 0.7239 - val_acc: 0.5200\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 277us/step - loss: 0.0983 - acc: 0.9900 - val_loss: 0.7588 - val_acc: 0.5200\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 0s 286us/step - loss: 0.0912 - acc: 1.0000 - val_loss: 1.2337 - val_acc: 0.5100\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 0s 288us/step - loss: 0.1434 - acc: 0.9600 - val_loss: 0.8301 - val_acc: 0.5300\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 0s 287us/step - loss: 0.0444 - acc: 1.0000 - val_loss: 0.7265 - val_acc: 0.5100\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}